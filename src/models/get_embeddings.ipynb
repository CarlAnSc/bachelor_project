{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlschmidt/anaconda3/envs/colo-repo/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torchvision.models.resnet as resnet\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from lazypredict.Supervised import LazyClassifier\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.005077046405985571\n"
     ]
    }
   ],
   "source": [
    "val_preds  = pickle.load(open('../../val_preds.pkl', 'rb'))\n",
    "\n",
    "val_img_preds = []\n",
    "val_img_truth = []\n",
    "\n",
    "for i in val_preds.keys():\n",
    "   val_img_preds.append(torch.argmax(torch.from_numpy(val_preds[i][0]), dim=1))\n",
    "   val_img_truth.append(torch.from_numpy(val_preds[i][2]))\n",
    "\n",
    "all_truths = torch.cat(val_img_truth).numpy()\n",
    "all_preds = torch.cat(val_img_preds).numpy()\n",
    "\n",
    "#accuracy\n",
    "print('Accuracy: ', sklearn.metrics.accuracy_score(all_truths, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/annotations/imagenet_x_train_multi_factor.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UseMetaData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m annotation_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../../data/annotations/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      5\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/mnt/f/MetalabelIntegration/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m train_data \u001b[39m=\u001b[39m UseMetaData(\n\u001b[1;32m      8\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, path, annotation_path, transform\u001b[39m=\u001b[39mValTransforms()\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m val_data \u001b[39m=\u001b[39m UseMetaData(\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m, path, annotation_path, transform\u001b[39m=\u001b[39mValTransforms())\n\u001b[1;32m     12\u001b[0m number_of_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_data\u001b[39m.\u001b[39mclasses)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'UseMetaData' is not defined"
     ]
    }
   ],
   "source": [
    "dotenvpath = find_dotenv()\n",
    "load_dotenv(dotenvpath)\n",
    "\n",
    "annotation_path = '../../data/annotations/'\n",
    "path = '/mnt/f/MetalabelIntegration/'\n",
    "\n",
    "train_data = UseMetaData(\n",
    "        \"train\", path, annotation_path, transform=ValTransforms()\n",
    "    )\n",
    "val_data = UseMetaData(\"val\", path, annotation_path, transform=ValTransforms())\n",
    "    \n",
    "number_of_classes = len(train_data.classes)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=8,\n",
    "        num_workers=8,\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=8,\n",
    "        num_workers=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.3473, -1.3987, -1.3987,  ..., -1.2103, -1.2274, -1.2274],\n",
       "          [-1.2959, -1.3130, -1.3130,  ..., -1.2103, -1.1760, -1.1589],\n",
       "          [-1.2788, -1.2788, -1.2445,  ..., -1.2103, -1.1760, -1.1418],\n",
       "          ...,\n",
       "          [-2.0665, -2.1008, -2.0837,  ..., -1.8953, -1.9124, -1.8953],\n",
       "          [-2.0837, -2.0837, -2.1008,  ..., -1.8610, -1.8610, -1.8439],\n",
       "          [-2.1008, -2.1008, -2.1008,  ..., -1.8610, -1.8439, -1.8610]],\n",
       " \n",
       "         [[ 1.0805,  1.0805,  1.0980,  ..., -0.6001, -0.6001, -0.6001],\n",
       "          [ 1.1155,  1.1506,  1.1506,  ..., -0.6176, -0.6001, -0.5826],\n",
       "          [ 1.1331,  1.1506,  1.1155,  ..., -0.6176, -0.6001, -0.5476],\n",
       "          ...,\n",
       "          [-2.0357, -2.0357, -2.0182,  ..., -1.9132, -1.8782, -1.9307],\n",
       "          [-2.0357, -2.0182, -2.0357,  ..., -1.9657, -1.9307, -1.9482],\n",
       "          [-2.0357, -2.0182, -2.0357,  ..., -2.0182, -2.0182, -2.0182]],\n",
       " \n",
       "         [[ 1.4025,  1.3677,  1.3677,  ...,  0.1302,  0.1302,  0.1476],\n",
       "          [ 1.4025,  1.4200,  1.4200,  ...,  0.1302,  0.1476,  0.1476],\n",
       "          [ 1.4200,  1.4200,  1.4200,  ...,  0.1302,  0.1302,  0.1825],\n",
       "          ...,\n",
       "          [-1.5081, -1.5256, -1.5430,  ..., -1.2816, -1.2816, -1.3164],\n",
       "          [-1.5081, -1.5081, -1.5430,  ..., -1.3339, -1.2990, -1.3164],\n",
       "          [-1.5256, -1.5081, -1.5256,  ..., -1.3861, -1.3687, -1.3861]]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.]),\n",
       " 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader.dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "print(len(val_loader.dataset.class_to_idx))\n",
    "print(len(train_loader.dataset.class_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.0777,  2.0777,  2.0948,  ...,  2.1462,  2.1804,  2.1633],\n",
       "          [ 2.1119,  2.0948,  2.0948,  ...,  2.1290,  2.1633,  2.1462],\n",
       "          [ 2.1290,  2.0948,  2.1119,  ...,  2.1462,  2.1290,  2.1804],\n",
       "          ...,\n",
       "          [-0.3369, -0.2684, -0.1828,  ...,  0.9646,  0.9817,  1.0159],\n",
       "          [-0.5938, -0.4397, -0.2513,  ...,  0.9646,  0.9817,  0.9132],\n",
       "          [-1.8439, -1.5357, -1.1932,  ...,  1.4269,  1.5297,  1.0331]],\n",
       " \n",
       "         [[ 2.3936,  2.3936,  2.4111,  ...,  2.0434,  2.0959,  2.0959],\n",
       "          [ 2.3936,  2.3936,  2.3936,  ...,  2.0434,  2.0784,  2.0784],\n",
       "          [ 2.3936,  2.3936,  2.3761,  ...,  1.9909,  2.0259,  2.0084],\n",
       "          ...,\n",
       "          [-0.0224, -0.0224,  0.0301,  ...,  0.8004,  0.8004,  0.8179],\n",
       "          [-0.2150, -0.0224,  0.0826,  ...,  0.8529,  0.7829,  0.8179],\n",
       "          [-1.6331, -1.3004, -0.8978,  ...,  1.2206,  1.1155,  0.8529]],\n",
       " \n",
       "         [[ 2.2740,  2.1868,  2.2566,  ...,  1.7860,  1.8383,  1.8557],\n",
       "          [ 2.2914,  2.3263,  2.3263,  ...,  1.7337,  1.8208,  1.8383],\n",
       "          [ 2.2043,  2.2914,  2.4134,  ...,  1.6814,  1.7685,  1.7685],\n",
       "          ...,\n",
       "          [ 0.3568,  0.4091,  0.5311,  ...,  0.8099,  0.7576,  0.7751],\n",
       "          [ 0.3045,  0.4439,  0.6531,  ...,  0.7576,  0.7402,  0.7751],\n",
       "          [-0.9853, -0.5670, -0.0964,  ...,  0.8971,  1.0017,  0.8274]]]),\n",
       " tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.]),\n",
       " 572,\n",
       " '/mnt/f/MetalabelIntegration/val/n03443371/n03443371_18813.JPEG')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader.dataset[6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 16)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(val_dict[0][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kan ikke huske er embeddings lavet med et forward pass på de 50.000 billeder?**\n",
    "**Måske stadig spørge Nicki om hans approach med at lave embeddings på 50.000 images, for derefter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict = pickle.load(open('../../data/train_embeddings.pkl', 'rb')) # De 50.000 billeder\n",
    "#val_dict = pickle.load(open('../../data/val_embeddings.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3433844 , 0.17441927, 1.7837598 , ..., 0.07956456, 0.11728598,\n",
       "        0.83721125],\n",
       "       [0.7219721 , 1.2102747 , 0.7870118 , ..., 0.3597971 , 0.14651966,\n",
       "        0.45646465],\n",
       "       [0.05431684, 0.09599383, 0.3473871 , ..., 0.24399187, 0.37392047,\n",
       "        0.08033226],\n",
       "       ...,\n",
       "       [0.12876727, 0.32736042, 0.0716015 , ..., 0.18868539, 0.6687686 ,\n",
       "        0.2971885 ],\n",
       "       [0.0243606 , 0.2680786 , 0.10872113, ..., 0.03809149, 0.        ,\n",
       "        0.3502503 ],\n",
       "       [0.20314777, 0.14395699, 0.13556641, ..., 0.1184201 , 0.51048464,\n",
       "        0.47193465]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dict[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img_data = []\n",
    "# train_meta_data = []\n",
    "# train_labels = []\n",
    "\n",
    "val_img_data = []\n",
    "val_meta_data = []\n",
    "val_labels = []\n",
    "\n",
    "# for i in train_dict.keys():\n",
    "#     train_img_data.append(torch.from_numpy(train_dict[i][0]))\n",
    "#     train_meta_data.append(torch.from_numpy(train_dict[i][1]))\n",
    "#     train_labels.append(torch.from_numpy(train_dict[i][2]))\n",
    "\n",
    "for i in val_dict.keys():\n",
    "    val_img_data.append(torch.from_numpy(val_dict[i][0]))\n",
    "    val_meta_data.append(torch.from_numpy(val_dict[i][1]))\n",
    "    val_labels.append(torch.from_numpy(val_dict[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img_data = torch.cat(train_img_data, 0).detach()\n",
    "# train_meta_data = torch.cat(train_meta_data, 0)\n",
    "# train_cat_data = torch.cat([train_img_data, train_meta_data], 1).numpy()\n",
    "# train_labels = torch.cat(train_labels, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_data = torch.cat(val_img_data, 0).detach()\n",
    "val_meta_data = torch.cat(val_meta_data, 0)\n",
    "val_cat_data = torch.cat([val_img_data, val_meta_data], 1).numpy()\n",
    "val_labels = torch.cat(val_labels, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_cat_data = np.concatenate([train_cat_data, val_cat_data], 0)\n",
    "# all_labels = np.concatenate([train_labels, val_labels], 0)\n",
    "# all_img_data = np.concatenate([train_img_data, val_img_data], 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using k=?-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(classifier):\n",
    "\n",
    "    clf_cat = classifier()\n",
    "    clf_img = classifier()\n",
    "\n",
    "    clf_cat.fit(X_train_cat, y_train_cat)\n",
    "\n",
    "    y_pred_cat = clf_cat.predict(X_test_cat)\n",
    "\n",
    "    accuracy_cat = sklearn.metrics.accuracy_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    print('Accuracy cat: ', sklearn.metrics.accuracy_score(y_test_cat, y_pred_cat))\n",
    "\n",
    "    return clf_cat, clf_img, y_pred_cat, accuracy_cat\n",
    "\n",
    "def test_classifier_fold(classifier, X_train_cat, y_train_cat, X_test_cat, y_test_cat, X_train_img, y_train_img, X_test_img, y_test_img):\n",
    "\n",
    "    clf_cat = classifier()\n",
    "    clf_img = classifier()\n",
    "\n",
    "    clf_cat.fit(X_train_cat, y_train_cat)\n",
    "    clf_img.fit(X_train_img, y_train_img)\n",
    "\n",
    "\n",
    "    y_pred_cat = clf_cat.predict(X_test_cat)\n",
    "    y_pred_img = clf_img.predict(X_test_img)\n",
    "\n",
    "    accuracy_cat = sklearn.metrics.accuracy_score(y_test_cat, y_pred_cat)\n",
    "    accuracy_img = sklearn.metrics.accuracy_score(y_test_img, y_pred_img)\n",
    "\n",
    "    print('Accuracy cat: ', sklearn.metrics.accuracy_score(y_test_cat, y_pred_cat))\n",
    "    print('Accuracy img: ', sklearn.metrics.accuracy_score(y_test_img, y_pred_img))\n",
    "\n",
    "    return clf_cat, clf_img, y_pred_cat, y_pred_img, accuracy_cat, accuracy_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=None, shuffle=False)\n",
      "Fold 1:\n",
      "Accuracy cat:  0.6521383261714754\n",
      "Accuracy img:  0.6513198281154082\n",
      "[[6341   33]\n",
      " [  25 3375]]\n",
      "Fold 2:\n",
      "Accuracy cat:  0.6601186822181297\n",
      "Accuracy img:  0.6582770615919787\n",
      "[[6415   37]\n",
      " [  19 3303]]\n",
      "Fold 3:\n",
      "Accuracy cat:  0.6657458563535912\n",
      "Accuracy img:  0.6651319828115408\n",
      "[[6480   27]\n",
      " [  21 3246]]\n",
      "Fold 4:\n",
      "Accuracy cat:  0.6619257137010129\n",
      "Accuracy img:  0.6617210682492581\n",
      "[[6438   31]\n",
      " [  29 3275]]\n",
      "Fold 5:\n",
      "Accuracy cat:  0.665097718203213\n",
      "Accuracy img:  0.6642791363961936\n",
      "[[6473   27]\n",
      " [  19 3254]]\n",
      "[[32147.   155.]\n",
      " [  113. 16453.]]\n",
      "   Using Meta  Just images     Delta  McNemar p-value\n",
      "0    0.652138     0.651320  0.000818         0.358020\n",
      "1    0.660119     0.658277  0.001842         0.019208\n",
      "2    0.665746     0.665132  0.000614         0.014868\n",
      "3    0.661926     0.661721  0.000205         0.026773\n",
      "4    0.665098     0.664279  0.000819         0.012263\n",
      "------------Cummulated McNemar test------------\n",
      "(6.272388059701493, 0.012263375366217383)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "from mlxtend.evaluate import mcnemar\n",
    "\n",
    "\n",
    "\n",
    "X = val_cat_data\n",
    "\n",
    "N_splits = 5\n",
    "kf = KFold(n_splits=N_splits)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "print(kf)\n",
    "\n",
    "df_acc = pd.DataFrame(columns=['Using Meta', 'Just images', 'Delta', 'McNemar p-value'])\n",
    "M_table_cumm = np.zeros((2,2))\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(f\"Fold {i + 1}:\")\n",
    "    _,_, y_pred_cat, y_pred_img, acc_cat, acc_img = test_classifier_fold(classifier=KNeighborsClassifier,\n",
    "                                                      X_train_cat = val_cat_data[train_index],\n",
    "                                                        y_train_cat = val_labels[train_index],\n",
    "                                                          X_test_cat = val_cat_data[test_index],\n",
    "                                                            y_test_cat = val_labels[test_index], \n",
    "                                                              X_train_img = val_img_data[train_index],\n",
    "                                                                y_train_img = val_labels[train_index],\n",
    "                                                                  X_test_img = val_img_data[test_index],\n",
    "                                                                    y_test_img = val_labels[test_index])\n",
    "    # Run a mcnemar test for the two classifiers\n",
    "    M_table = mcnemar_table(y_target= val_labels[test_index],\n",
    "                             y_model1 = y_pred_cat,\n",
    "                              y_model2 = y_pred_img)\n",
    "    print(M_table)\n",
    "    M_table_cumm += M_table\n",
    "    chi2, p = mcnemar(ary=M_table_cumm, corrected=True)\n",
    "    # append accuracies to df_acc\n",
    "    df_acc.loc[i] = [acc_cat, acc_img, acc_cat- acc_img, p]\n",
    "print(M_table_cumm)\n",
    "print(df_acc)\n",
    "print('------------Cummulated McNemar test------------')\n",
    "print(mcnemar(ary=M_table_cumm, corrected=True))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using k=1-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(val_cat_data, val_labels, test_size=0.2, random_state=42)\n",
    "X_train_img, X_test_img, y_train_img, y_test_img = train_test_split(val_img_data, val_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# https://stackoverflow.com/questions/49134338/kfolds-cross-validation-vs-train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy cat:  0.6633926744423982\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clf_cat, clf_img, y_pred_cat, y_pred_img, accuracy_cat, accuracy_img \u001b[39m=\u001b[39m test_classifier(KNeighborsClassifier)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 4)"
     ]
    }
   ],
   "source": [
    "clf_cat, clf_img, y_pred_cat, y_pred_img, accuracy_cat, accuracy_img = test_classifier(KNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy cat:  0.7200736648250461\n",
      "Accuracy img:  0.7160834868017188\n"
     ]
    }
   ],
   "source": [
    "clf_cat, clf_img, y_pred_cat, y_pred_img, accuracy_cat, accuracy_img = test_classifier(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_cat, clf_img, y_pred_cat, y_pred_img, accuracy_cat, accuracy_img = test_classifier(xgb.XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier 1 - cat\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_cat, clf_img, y_pred_cat, y_pred_img, accuracy_cat, accuracy_img = test_classifier(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_cat, clf_img, y_pred_cat, y_pred_img, accuracy_cat, accuracy_img = test_classifier(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006145898280929901\n"
     ]
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(train_img_data, train_labels)\n",
    "preds = classifier.predict(val_img_data)\n",
    "print((preds == val_labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(val_cat_data[:1000], val_labels[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([524, 804, 826, ..., 524, 175, 495])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(val_cat_data[1000:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998772202668413\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(val_cat_data)\n",
    "print((preds == val_labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994474912007858\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(val_img_data, val_labels)\n",
    "preds = clf.predict(val_img_data)\n",
    "print((preds == val_labels).mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mensemble\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m      4\u001b[0m classifier \u001b[39m=\u001b[39m RandomForestClassifier(max_depth\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m classifier\u001b[39m.\u001b[39;49mfit(X_train_img, y_train_img)\n\u001b[1;32m      6\u001b[0m preds \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39mpredict(X_test_img)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m((preds \u001b[39m==\u001b[39m y_test_img)\u001b[39m.\u001b[39mmean())\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[1;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    465\u001b[0m ]\n\u001b[1;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    476\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    477\u001b[0m )(\n\u001b[1;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    479\u001b[0m         t,\n\u001b[1;32m    480\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[1;32m    481\u001b[0m         X,\n\u001b[1;32m    482\u001b[0m         y,\n\u001b[1;32m    483\u001b[0m         sample_weight,\n\u001b[1;32m    484\u001b[0m         i,\n\u001b[1;32m    485\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[1;32m    486\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    487\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    488\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[1;32m    489\u001b[0m     )\n\u001b[1;32m    490\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    182\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[0;32m--> 184\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/sklearn/tree/_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    890\u001b[0m         X,\n\u001b[1;32m    891\u001b[0m         y,\n\u001b[1;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "classifier.fit(X_train_img, y_train_img)\n",
    "preds = classifier.predict(X_test_img)\n",
    "print((preds == y_test_img).mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "for train_data in tqdm([train_img_data, train_cat_data]):\n",
    "    classifier = DecisionTreeClassifier(random_state=0)\n",
    "    classifier.fit(train_data.numpy(), train_labels.numpy())\n",
    "    preds = classifier.predict(train_data.numpy())\n",
    "    acc = (preds == train_labels.numpy()).mean()\n",
    "    print(acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [08:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m classifier \u001b[39m=\u001b[39m GaussianNB()\n\u001b[1;32m      4\u001b[0m classifier\u001b[39m.\u001b[39mfit(train_data\u001b[39m.\u001b[39mnumpy(), train_labels\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m----> 5\u001b[0m preds \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mpredict(train_data\u001b[39m.\u001b[39;49mnumpy())\n\u001b[1;32m      6\u001b[0m acc \u001b[39m=\u001b[39m (preds \u001b[39m==\u001b[39m train_labels\u001b[39m.\u001b[39mnumpy())\u001b[39m.\u001b[39mmean()\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(acc)\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/sklearn/naive_bayes.py:106\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    104\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m    105\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_X(X)\n\u001b[0;32m--> 106\u001b[0m jll \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_joint_log_likelihood(X)\n\u001b[1;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_[np\u001b[39m.\u001b[39margmax(jll, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)]\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/sklearn/naive_bayes.py:514\u001b[0m, in \u001b[0;36mGaussianNB._joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    512\u001b[0m     jointi \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_prior_[i])\n\u001b[1;32m    513\u001b[0m     n_ij \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mlog(\u001b[39m2.0\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mpi \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvar_[i, :]))\n\u001b[0;32m--> 514\u001b[0m     n_ij \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49msum(((X \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtheta_[i, :]) \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m) \u001b[39m/\u001b[39;49m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvar_[i, :]), \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    515\u001b[0m     joint_log_likelihood\u001b[39m.\u001b[39mappend(jointi \u001b[39m+\u001b[39m n_ij)\n\u001b[1;32m    517\u001b[0m joint_log_likelihood \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(joint_log_likelihood)\u001b[39m.\u001b[39mT\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2183\u001b[0m, in \u001b[0;36m_sum_dispatcher\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2114\u001b[0m \u001b[39m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2115\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2178\u001b[0m \n\u001b[1;32m   2179\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2180\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m'\u001b[39m, a_min, a_max, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 2183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sum_dispatcher\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2184\u001b[0m                     initial\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, where\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2185\u001b[0m     \u001b[39mreturn\u001b[39;00m (a, out)\n\u001b[1;32m   2188\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[1;32m   2189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[1;32m   2190\u001b[0m         initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "for train_data in tqdm([train_img_data, train_cat_data]):\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(train_data.numpy(), train_labels.numpy())\n",
    "    preds = classifier.predict(train_data.numpy())\n",
    "    acc = (preds == train_labels.numpy()).mean()\n",
    "    print(acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X = val_cat_data   #train_cat_data\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(X)\n",
    "\n",
    "sns.scatterplot(pca.explained_variance_ratio_*100)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio vs Principal Component')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68838257"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)[99]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting metalabel tabular data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenvpath = find_dotenv()\n",
    "load_dotenv(dotenvpath)\n",
    "\n",
    "annotation_path = \"../../data/annotations/\"\n",
    "path = '/mnt/f/MetalabelIntegration/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = UseMetaData(\n",
    "        \"train\", path, annotation_path, transform=ValTransforms()\n",
    "    )\n",
    "val_data = UseMetaData(\"val\", path, annotation_path, transform=ValTransforms())\n",
    "    \n",
    "number_of_classes = len(train_data.classes)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=16,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=16,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/juliu/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load(\n",
    "                \"pytorch/vision:v0.9.0\",\n",
    "                \"resnet50\",\n",
    "                weights=\"ResNet50_Weights.IMAGENET1K_V1\",\n",
    "            )\n",
    "model.fc = torch.nn.Identity()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_data = []\n",
    "train_meta_data = []\n",
    "train_labels = []\n",
    "\n",
    "for i, batch in enumerate(tqdm.tqdm(train_loader)):\n",
    "    train_img_data.append(model(batch[0]))\n",
    "    train_meta_data.append(batch[1])\n",
    "    train_labels.append(batch[2])\n",
    "    if i > 100:\n",
    "        break\n",
    "\n",
    "train_img_data = torch.cat(train_img_data, 0).detach()\n",
    "train_meta_data = torch.cat(train_meta_data, 0)\n",
    "train_cat_data = torch.cat([train_img_data, train_meta_data], 1)\n",
    "train_labels = torch.cat(train_labels, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Using Meta</th>\n",
       "      <th>Just images</th>\n",
       "      <th>Delta</th>\n",
       "      <th>McNemar p-value</th>\n",
       "      <th>Final p_1</th>\n",
       "      <th>Final theta_2</th>\n",
       "      <th>Final CI_2</th>\n",
       "      <th>Final p_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.663290</td>\n",
       "      <td>0.662165</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.145120</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>(0.0009806031607664245, 0.002334448885514817)</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.662370</td>\n",
       "      <td>0.659300</td>\n",
       "      <td>0.003069</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>(0.0009806031607664245, 0.002334448885514817)</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.669224</td>\n",
       "      <td>0.667894</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.090559</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>(0.0009806031607664245, 0.002334448885514817)</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.655684</td>\n",
       "      <td>0.654968</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.307228</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>(0.0009806031607664245, 0.002334448885514817)</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.660084</td>\n",
       "      <td>0.658037</td>\n",
       "      <td>0.002046</td>\n",
       "      <td>0.006496</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>(0.0009806031607664245, 0.002334448885514817)</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Using Meta  Just images     Delta  McNemar p-value  Final p_1  \\\n",
       "1    0.663290     0.662165  0.001125         0.145120   0.000002   \n",
       "2    0.662370     0.659300  0.003069         0.000275   0.000002   \n",
       "3    0.669224     0.667894  0.001330         0.090559   0.000002   \n",
       "4    0.655684     0.654968  0.000716         0.307228   0.000002   \n",
       "5    0.660084     0.658037  0.002046         0.006496   0.000002   \n",
       "\n",
       "   Final theta_2                                     Final CI_2  Final p_2  \n",
       "1       0.001658  (0.0009806031607664245, 0.002334448885514817)   0.000002  \n",
       "2       0.001658  (0.0009806031607664245, 0.002334448885514817)   0.000002  \n",
       "3       0.001658  (0.0009806031607664245, 0.002334448885514817)   0.000002  \n",
       "4       0.001658  (0.0009806031607664245, 0.002334448885514817)   0.000002  \n",
       "5       0.001658  (0.0009806031607664245, 0.002334448885514817)   0.000002  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../k-fold/k-nearest_mcnemar.csv', index_col=0)\n",
    "df.index = df.index + 1\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  Using Meta &  Just images &     Delta \\\\\n",
      "\\midrule\n",
      "1 &    0.663290 &     0.662165 &  0.001125 \\\\\n",
      "2 &    0.662370 &     0.659300 &  0.003069 \\\\\n",
      "3 &    0.669224 &     0.667894 &  0.001330 \\\\\n",
      "4 &    0.655684 &     0.654968 &  0.000716 \\\\\n",
      "5 &    0.660084 &     0.658037 &  0.002046 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/26n95fy50m782tbz32d5_1g40000gn/T/ipykernel_78504/1544154714.py:2: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(df.iloc[:,:3].to_latex())\n"
     ]
    }
   ],
   "source": [
    "# format latex table nicely\n",
    "print(df.iloc[:,:3].to_latex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format latex table nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['Using Meta'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = df['Just images'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0016575150954286633"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00067691,  0.00067693])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0.0009806031607664245, 0.002334448885514817]) -0.0016575150954286633\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6604729487460295"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6621304638414581"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  6\n",
       "1  2  5\n",
       "2  3  4\n",
       "3  4  3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip([1,2,3,4],[6,5,4,3]), columns=['a','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colo-repo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bcaae77deccda659b7240224c77c41f475b2ace73746821b6a2f4d3d41cec83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
