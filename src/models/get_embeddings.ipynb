{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlschmidt/anaconda3/envs/colo-repo/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torchvision.models.resnet as resnet\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from lazypredict.Supervised import LazyClassifier\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.005077046405985571\n"
     ]
    }
   ],
   "source": [
    "val_preds  = pickle.load(open('../../val_preds.pkl', 'rb'))\n",
    "\n",
    "val_img_preds = []\n",
    "val_img_truth = []\n",
    "\n",
    "for i in val_preds.keys():\n",
    "   val_img_preds.append(torch.argmax(torch.from_numpy(val_preds[i][0]), dim=1))\n",
    "   val_img_truth.append(torch.from_numpy(val_preds[i][2]))\n",
    "\n",
    "all_truths = torch.cat(val_img_truth).numpy()\n",
    "all_preds = torch.cat(val_img_preds).numpy()\n",
    "\n",
    "#accuracy\n",
    "print('Accuracy: ', sklearn.metrics.accuracy_score(all_truths, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/annotations/imagenet_x_train_multi_factor.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UseMetaData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m annotation_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../../data/annotations/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      5\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/mnt/f/MetalabelIntegration/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m train_data \u001b[39m=\u001b[39m UseMetaData(\n\u001b[1;32m      8\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, path, annotation_path, transform\u001b[39m=\u001b[39mValTransforms()\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m val_data \u001b[39m=\u001b[39m UseMetaData(\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m, path, annotation_path, transform\u001b[39m=\u001b[39mValTransforms())\n\u001b[1;32m     12\u001b[0m number_of_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_data\u001b[39m.\u001b[39mclasses)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'UseMetaData' is not defined"
     ]
    }
   ],
   "source": [
    "dotenvpath = find_dotenv()\n",
    "load_dotenv(dotenvpath)\n",
    "\n",
    "annotation_path = '../../data/annotations/'\n",
    "path = '/mnt/f/MetalabelIntegration/'\n",
    "\n",
    "train_data = UseMetaData(\n",
    "        \"train\", path, annotation_path, transform=ValTransforms()\n",
    "    )\n",
    "val_data = UseMetaData(\"val\", path, annotation_path, transform=ValTransforms())\n",
    "    \n",
    "number_of_classes = len(train_data.classes)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=8,\n",
    "        num_workers=8,\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=8,\n",
    "        num_workers=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.3473, -1.3987, -1.3987,  ..., -1.2103, -1.2274, -1.2274],\n",
       "          [-1.2959, -1.3130, -1.3130,  ..., -1.2103, -1.1760, -1.1589],\n",
       "          [-1.2788, -1.2788, -1.2445,  ..., -1.2103, -1.1760, -1.1418],\n",
       "          ...,\n",
       "          [-2.0665, -2.1008, -2.0837,  ..., -1.8953, -1.9124, -1.8953],\n",
       "          [-2.0837, -2.0837, -2.1008,  ..., -1.8610, -1.8610, -1.8439],\n",
       "          [-2.1008, -2.1008, -2.1008,  ..., -1.8610, -1.8439, -1.8610]],\n",
       " \n",
       "         [[ 1.0805,  1.0805,  1.0980,  ..., -0.6001, -0.6001, -0.6001],\n",
       "          [ 1.1155,  1.1506,  1.1506,  ..., -0.6176, -0.6001, -0.5826],\n",
       "          [ 1.1331,  1.1506,  1.1155,  ..., -0.6176, -0.6001, -0.5476],\n",
       "          ...,\n",
       "          [-2.0357, -2.0357, -2.0182,  ..., -1.9132, -1.8782, -1.9307],\n",
       "          [-2.0357, -2.0182, -2.0357,  ..., -1.9657, -1.9307, -1.9482],\n",
       "          [-2.0357, -2.0182, -2.0357,  ..., -2.0182, -2.0182, -2.0182]],\n",
       " \n",
       "         [[ 1.4025,  1.3677,  1.3677,  ...,  0.1302,  0.1302,  0.1476],\n",
       "          [ 1.4025,  1.4200,  1.4200,  ...,  0.1302,  0.1476,  0.1476],\n",
       "          [ 1.4200,  1.4200,  1.4200,  ...,  0.1302,  0.1302,  0.1825],\n",
       "          ...,\n",
       "          [-1.5081, -1.5256, -1.5430,  ..., -1.2816, -1.2816, -1.3164],\n",
       "          [-1.5081, -1.5081, -1.5430,  ..., -1.3339, -1.2990, -1.3164],\n",
       "          [-1.5256, -1.5081, -1.5256,  ..., -1.3861, -1.3687, -1.3861]]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.]),\n",
       " 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader.dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "print(len(val_loader.dataset.class_to_idx))\n",
    "print(len(train_loader.dataset.class_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.0777,  2.0777,  2.0948,  ...,  2.1462,  2.1804,  2.1633],\n",
       "          [ 2.1119,  2.0948,  2.0948,  ...,  2.1290,  2.1633,  2.1462],\n",
       "          [ 2.1290,  2.0948,  2.1119,  ...,  2.1462,  2.1290,  2.1804],\n",
       "          ...,\n",
       "          [-0.3369, -0.2684, -0.1828,  ...,  0.9646,  0.9817,  1.0159],\n",
       "          [-0.5938, -0.4397, -0.2513,  ...,  0.9646,  0.9817,  0.9132],\n",
       "          [-1.8439, -1.5357, -1.1932,  ...,  1.4269,  1.5297,  1.0331]],\n",
       " \n",
       "         [[ 2.3936,  2.3936,  2.4111,  ...,  2.0434,  2.0959,  2.0959],\n",
       "          [ 2.3936,  2.3936,  2.3936,  ...,  2.0434,  2.0784,  2.0784],\n",
       "          [ 2.3936,  2.3936,  2.3761,  ...,  1.9909,  2.0259,  2.0084],\n",
       "          ...,\n",
       "          [-0.0224, -0.0224,  0.0301,  ...,  0.8004,  0.8004,  0.8179],\n",
       "          [-0.2150, -0.0224,  0.0826,  ...,  0.8529,  0.7829,  0.8179],\n",
       "          [-1.6331, -1.3004, -0.8978,  ...,  1.2206,  1.1155,  0.8529]],\n",
       " \n",
       "         [[ 2.2740,  2.1868,  2.2566,  ...,  1.7860,  1.8383,  1.8557],\n",
       "          [ 2.2914,  2.3263,  2.3263,  ...,  1.7337,  1.8208,  1.8383],\n",
       "          [ 2.2043,  2.2914,  2.4134,  ...,  1.6814,  1.7685,  1.7685],\n",
       "          ...,\n",
       "          [ 0.3568,  0.4091,  0.5311,  ...,  0.8099,  0.7576,  0.7751],\n",
       "          [ 0.3045,  0.4439,  0.6531,  ...,  0.7576,  0.7402,  0.7751],\n",
       "          [-0.9853, -0.5670, -0.0964,  ...,  0.8971,  1.0017,  0.8274]]]),\n",
       " tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.]),\n",
       " 572,\n",
       " '/mnt/f/MetalabelIntegration/val/n03443371/n03443371_18813.JPEG')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader.dataset[6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 16)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(val_dict[0][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kan ikke huske er embeddings lavet med et forward pass på de 50.000 billeder?**\n",
    "**Måske stadig spørge Nicki om hans approach med at lave embeddings på 50.000 images, for derefter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict = pickle.load(open('../../data/train_embeddings.pkl', 'rb')) # De 50.000 billeder\n",
    "#val_dict = pickle.load(open('../../data/val_embeddings.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3433844 , 0.17441927, 1.7837598 , ..., 0.07956456, 0.11728598,\n",
       "        0.83721125],\n",
       "       [0.7219721 , 1.2102747 , 0.7870118 , ..., 0.3597971 , 0.14651966,\n",
       "        0.45646465],\n",
       "       [0.05431684, 0.09599383, 0.3473871 , ..., 0.24399187, 0.37392047,\n",
       "        0.08033226],\n",
       "       ...,\n",
       "       [0.12876727, 0.32736042, 0.0716015 , ..., 0.18868539, 0.6687686 ,\n",
       "        0.2971885 ],\n",
       "       [0.0243606 , 0.2680786 , 0.10872113, ..., 0.03809149, 0.        ,\n",
       "        0.3502503 ],\n",
       "       [0.20314777, 0.14395699, 0.13556641, ..., 0.1184201 , 0.51048464,\n",
       "        0.47193465]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dict[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img_data = []\n",
    "# train_meta_data = []\n",
    "# train_labels = []\n",
    "\n",
    "val_img_data = []\n",
    "val_meta_data = []\n",
    "val_labels = []\n",
    "\n",
    "# for i in train_dict.keys():\n",
    "#     train_img_data.append(torch.from_numpy(train_dict[i][0]))\n",
    "#     train_meta_data.append(torch.from_numpy(train_dict[i][1]))\n",
    "#     train_labels.append(torch.from_numpy(train_dict[i][2]))\n",
    "\n",
    "for i in val_dict.keys():\n",
    "    val_img_data.append(torch.from_numpy(val_dict[i][0]))\n",
    "    val_meta_data.append(torch.from_numpy(val_dict[i][1]))\n",
    "    val_labels.append(torch.from_numpy(val_dict[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img_data = torch.cat(train_img_data, 0).detach()\n",
    "# train_meta_data = torch.cat(train_meta_data, 0)\n",
    "# train_cat_data = torch.cat([train_img_data, train_meta_data], 1).numpy()\n",
    "# train_labels = torch.cat(train_labels, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_data = torch.cat(val_img_data, 0).detach()\n",
    "val_meta_data = torch.cat(val_meta_data, 0)\n",
    "val_cat_data = torch.cat([val_img_data, val_meta_data], 1).numpy()\n",
    "val_labels = torch.cat(val_labels, 0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_cat_data = np.concatenate([train_cat_data, val_cat_data], 0)\n",
    "# all_labels = np.concatenate([train_labels, val_labels], 0)\n",
    "# all_img_data = np.concatenate([train_img_data, val_img_data], 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using k=?-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(classifier):\n",
    "\n",
    "    clf_cat = classifier()\n",
    "    clf_img = classifier()\n",
    "\n",
    "    clf_cat.fit(X_train_cat, y_train_cat)\n",
    "\n",
    "    y_pred_cat = clf_cat.predict(X_test_cat)\n",
    "\n",
    "    accuracy_cat = sklearn.metrics.accuracy_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    print('Accuracy cat: ', sklearn.metrics.accuracy_score(y_test_cat, y_pred_cat))\n",
    "\n",
    "    return clf_cat, clf_img, y_pred_cat, accuracy_cat\n",
    "\n",
    "def test_classifier_fold(classifier, X_train_cat, y_train_cat, X_test_cat, y_test_cat, X_train_img, y_train_img, X_test_img, y_test_img):\n",
    "\n",
    "    clf_cat = classifier()\n",
    "    clf_img = classifier()\n",
    "\n",
    "    clf_cat.fit(X_train_cat, y_train_cat)\n",
    "    clf_img.fit(X_train_img, y_train_img)\n",
    "\n",
    "\n",
    "    y_pred_cat = clf_cat.predict(X_test_cat)\n",
    "    y_pred_img = clf_img.predict(X_test_img)\n",
    "\n",
    "    accuracy_cat = sklearn.metrics.accuracy_score(y_test_cat, y_pred_cat)\n",
    "    accuracy_img = sklearn.metrics.accuracy_score(y_test_img, y_pred_img)\n",
    "\n",
    "    print('Accuracy cat: ', sklearn.metrics.accuracy_score(y_test_cat, y_pred_cat))\n",
    "    print('Accuracy img: ', sklearn.metrics.accuracy_score(y_test_img, y_pred_img))\n",
    "\n",
    "    return clf_cat, clf_img, y_pred_cat, y_pred_img, accuracy_cat, accuracy_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=None, shuffle=False)\n",
      "Fold 1:\n",
      "Accuracy cat:  0.6521383261714754\n",
      "Accuracy img:  0.6513198281154082\n",
      "[[6341   33]\n",
      " [  25 3375]]\n",
      "Fold 2:\n",
      "Accuracy cat:  0.6601186822181297\n",
      "Accuracy img:  0.6582770615919787\n",
      "[[6415   37]\n",
      " [  19 3303]]\n",
      "Fold 3:\n",
      "Accuracy cat:  0.6657458563535912\n",
      "Accuracy img:  0.6651319828115408\n",
      "[[6480   27]\n",
      " [  21 3246]]\n",
      "Fold 4:\n",
      "Accuracy cat:  0.6619257137010129\n",
      "Accuracy img:  0.6617210682492581\n",
      "[[6438   31]\n",
      " [  29 3275]]\n",
      "Fold 5:\n",
      "Accuracy cat:  0.665097718203213\n",
      "Accuracy img:  0.6642791363961936\n",
      "[[6473   27]\n",
      " [  19 3254]]\n",
      "[[32147.   155.]\n",
      " [  113. 16453.]]\n",
      "   Using Meta  Just images     Delta  McNemar p-value\n",
      "0    0.652138     0.651320  0.000818         0.358020\n",
      "1    0.660119     0.658277  0.001842         0.019208\n",
      "2    0.665746     0.665132  0.000614         0.014868\n",
      "3    0.661926     0.661721  0.000205         0.026773\n",
      "4    0.665098     0.664279  0.000819         0.012263\n",
      "------------Cummulated McNemar test------------\n",
      "(6.272388059701493, 0.012263375366217383)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "from mlxtend.evaluate import mcnemar\n",
    "\n",
    "\n",
    "\n",
    "X = val_cat_data\n",
    "\n",
    "N_splits = 5\n",
    "kf = KFold(n_splits=N_splits)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "print(kf)\n",
    "\n",
    "df_acc = pd.DataFrame(columns=['Using Meta', 'Just images', 'Delta', 'McNemar p-value'])\n",
    "M_table_cumm = np.zeros((2,2))\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(f\"Fold {i + 1}:\")\n",
    "    _,_, y_pred_cat, y_pred_img, acc_cat, acc_img = test_classifier_fold(classifier=KNeighborsClassifier,\n",
    "                                                      X_train_cat = val_cat_data[train_index],\n",
    "                                                        y_train_cat = val_labels[train_index],\n",
    "                                                          X_test_cat = val_cat_data[test_index],\n",
    "                                                            y_test_cat = val_labels[test_index], \n",
    "                                                              X_train_img = val_img_data[train_index],\n",
    "                                                                y_train_img = val_labels[train_index],\n",
    "                                                                  X_test_img = val_img_data[test_index],\n",
    "                                                                    y_test_img = val_labels[test_index])\n",
    "    # Run a mcnemar test for the two classifiers\n",
    "    M_table = mcnemar_table(y_target= val_labels[test_index],\n",
    "                             y_model1 = y_pred_cat,\n",
    "                              y_model2 = y_pred_img)\n",
    "    print(M_table)\n",
    "    M_table_cumm += M_table\n",
    "    chi2, p = mcnemar(ary=M_table_cumm, corrected=True)\n",
    "    # append accuracies to df_acc\n",
    "    df_acc.loc[i] = [acc_cat, acc_img, acc_cat- acc_img, p]\n",
    "print(M_table_cumm)\n",
    "print(df_acc)\n",
    "print('------------Cummulated McNemar test------------')\n",
    "print(mcnemar(ary=M_table_cumm, corrected=True))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using k=1-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(val_cat_data, val_labels, test_size=0.2, random_state=42)\n",
    "X_train_img, X_test_img, y_train_img, y_test_img = train_test_split(val_img_data, val_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# https://stackoverflow.com/questions/49134338/kfolds-cross-validation-vs-train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy cat:  0.6633926744423982\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clf_cat, clf_img, y_pred_cat, y_pred_img, accuracy_cat, accuracy_img \u001b[39m=\u001b[39m test_classifier(KNeighborsClassifier)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 4)"
     ]
    }
   ],
   "source": [
    "clf_cat, clf_img, y_pred_cat, y_pred_img, accuracy_cat, accuracy_img = test_classifier(KNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy cat:  0.7200736648250461\n",
      "Accuracy img:  0.7160834868017188\n"
     ]
    }
   ],
   "source": [
    "clf_cat, clf_img, y_pred_cat, y_pred_img, accuracy_cat, accuracy_img = test_classifier(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_cat, clf_img, y_pred_cat, y_pred_img, accuracy_cat, accuracy_img = test_classifier(xgb.XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier 1 - cat\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_cat, clf_img, y_pred_cat, y_pred_img, accuracy_cat, accuracy_img = test_classifier(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_cat, clf_img, y_pred_cat, y_pred_img, accuracy_cat, accuracy_img = test_classifier(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006145898280929901\n"
     ]
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(train_img_data, train_labels)\n",
    "preds = classifier.predict(val_img_data)\n",
    "print((preds == val_labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(val_cat_data[:1000], val_labels[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([524, 804, 826, ..., 524, 175, 495])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(val_cat_data[1000:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998772202668413\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(val_cat_data)\n",
    "print((preds == val_labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994474912007858\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(val_img_data, val_labels)\n",
    "preds = clf.predict(val_img_data)\n",
    "print((preds == val_labels).mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mensemble\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m      4\u001b[0m classifier \u001b[39m=\u001b[39m RandomForestClassifier(max_depth\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m classifier\u001b[39m.\u001b[39;49mfit(X_train_img, y_train_img)\n\u001b[1;32m      6\u001b[0m preds \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39mpredict(X_test_img)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m((preds \u001b[39m==\u001b[39m y_test_img)\u001b[39m.\u001b[39mmean())\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[1;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    465\u001b[0m ]\n\u001b[1;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    476\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    477\u001b[0m )(\n\u001b[1;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    479\u001b[0m         t,\n\u001b[1;32m    480\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[1;32m    481\u001b[0m         X,\n\u001b[1;32m    482\u001b[0m         y,\n\u001b[1;32m    483\u001b[0m         sample_weight,\n\u001b[1;32m    484\u001b[0m         i,\n\u001b[1;32m    485\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[1;32m    486\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    487\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    488\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[1;32m    489\u001b[0m     )\n\u001b[1;32m    490\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    182\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[0;32m--> 184\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/sklearn/tree/_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    890\u001b[0m         X,\n\u001b[1;32m    891\u001b[0m         y,\n\u001b[1;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "classifier.fit(X_train_img, y_train_img)\n",
    "preds = classifier.predict(X_test_img)\n",
    "print((preds == y_test_img).mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "for train_data in tqdm([train_img_data, train_cat_data]):\n",
    "    classifier = DecisionTreeClassifier(random_state=0)\n",
    "    classifier.fit(train_data.numpy(), train_labels.numpy())\n",
    "    preds = classifier.predict(train_data.numpy())\n",
    "    acc = (preds == train_labels.numpy()).mean()\n",
    "    print(acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [08:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m classifier \u001b[39m=\u001b[39m GaussianNB()\n\u001b[1;32m      4\u001b[0m classifier\u001b[39m.\u001b[39mfit(train_data\u001b[39m.\u001b[39mnumpy(), train_labels\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m----> 5\u001b[0m preds \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mpredict(train_data\u001b[39m.\u001b[39;49mnumpy())\n\u001b[1;32m      6\u001b[0m acc \u001b[39m=\u001b[39m (preds \u001b[39m==\u001b[39m train_labels\u001b[39m.\u001b[39mnumpy())\u001b[39m.\u001b[39mmean()\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(acc)\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/sklearn/naive_bayes.py:106\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    104\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m    105\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_X(X)\n\u001b[0;32m--> 106\u001b[0m jll \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_joint_log_likelihood(X)\n\u001b[1;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_[np\u001b[39m.\u001b[39margmax(jll, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)]\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/sklearn/naive_bayes.py:514\u001b[0m, in \u001b[0;36mGaussianNB._joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    512\u001b[0m     jointi \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_prior_[i])\n\u001b[1;32m    513\u001b[0m     n_ij \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mlog(\u001b[39m2.0\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mpi \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvar_[i, :]))\n\u001b[0;32m--> 514\u001b[0m     n_ij \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49msum(((X \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtheta_[i, :]) \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m) \u001b[39m/\u001b[39;49m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvar_[i, :]), \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    515\u001b[0m     joint_log_likelihood\u001b[39m.\u001b[39mappend(jointi \u001b[39m+\u001b[39m n_ij)\n\u001b[1;32m    517\u001b[0m joint_log_likelihood \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(joint_log_likelihood)\u001b[39m.\u001b[39mT\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/bach/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2183\u001b[0m, in \u001b[0;36m_sum_dispatcher\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2114\u001b[0m \u001b[39m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2115\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2178\u001b[0m \n\u001b[1;32m   2179\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2180\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m'\u001b[39m, a_min, a_max, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 2183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sum_dispatcher\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2184\u001b[0m                     initial\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, where\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2185\u001b[0m     \u001b[39mreturn\u001b[39;00m (a, out)\n\u001b[1;32m   2188\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[1;32m   2189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[1;32m   2190\u001b[0m         initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "for train_data in tqdm([train_img_data, train_cat_data]):\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(train_data.numpy(), train_labels.numpy())\n",
    "    preds = classifier.predict(train_data.numpy())\n",
    "    acc = (preds == train_labels.numpy()).mean()\n",
    "    print(acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X = train_meta_data   #train_cat_data\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X)\n",
    "\n",
    "sns.scatterplot(pca.explained_variance_ratio_*100)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio vs Principal Component')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting metalabel tabular data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenvpath = find_dotenv()\n",
    "load_dotenv(dotenvpath)\n",
    "\n",
    "annotation_path = \"../../data/annotations/\"\n",
    "path = '/mnt/f/MetalabelIntegration/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = UseMetaData(\n",
    "        \"train\", path, annotation_path, transform=ValTransforms()\n",
    "    )\n",
    "val_data = UseMetaData(\"val\", path, annotation_path, transform=ValTransforms())\n",
    "    \n",
    "number_of_classes = len(train_data.classes)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=16,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=16,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/juliu/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load(\n",
    "                \"pytorch/vision:v0.9.0\",\n",
    "                \"resnet50\",\n",
    "                weights=\"ResNet50_Weights.IMAGENET1K_V1\",\n",
    "            )\n",
    "model.fc = torch.nn.Identity()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_data = []\n",
    "train_meta_data = []\n",
    "train_labels = []\n",
    "\n",
    "for i, batch in enumerate(tqdm.tqdm(train_loader)):\n",
    "    train_img_data.append(model(batch[0]))\n",
    "    train_meta_data.append(batch[1])\n",
    "    train_labels.append(batch[2])\n",
    "    if i > 100:\n",
    "        break\n",
    "\n",
    "train_img_data = torch.cat(train_img_data, 0).detach()\n",
    "train_meta_data = torch.cat(train_meta_data, 0)\n",
    "train_cat_data = torch.cat([train_img_data, train_meta_data], 1)\n",
    "train_labels = torch.cat(train_labels, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colo-repo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bcaae77deccda659b7240224c77c41f475b2ace73746821b6a2f4d3d41cec83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
